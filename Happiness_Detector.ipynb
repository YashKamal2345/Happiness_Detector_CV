{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6Z74BllXZho",
        "outputId": "4b7f4d0a-fea3-4aee-8575-855df35ebfd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Camera capture failed: 'NoneType' object has no attribute 'split'\n"
          ]
        }
      ],
      "source": [
        "# üíª Complete Happy Face Detector Code for Google Colab (One Shell)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow # Specific for Colab\n",
        "import os\n",
        "\n",
        "# --- Configuration and Mock Data Setup ---\n",
        "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "num_classes = len(emotion_labels)\n",
        "img_rows, img_cols = 48, 48\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "model_filename = \"happy_face_detector_model.h5\"\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "# --- 1. Model Definition and Training (for a runnable example) ---\n",
        "\n",
        "def create_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Generate Mock Data\n",
        "print(\"üõ†Ô∏è Generating and training model with mock data for demonstration purposes...\")\n",
        "X_mock = np.random.rand(100, img_rows, img_cols, 1).astype('float32') / 255.0\n",
        "y_mock = np.random.randint(0, num_classes, 100)\n",
        "y_mock = to_categorical(y_mock, num_classes=num_classes)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_mock, y_mock, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and Save\n",
        "model = create_model(input_shape)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=2, verbose=0) # Minimal training\n",
        "model.save(model_filename)\n",
        "print(\"‚úÖ Model trained and saved.\")\n",
        "\n",
        "\n",
        "# --- 2. Camera Utility Function (JavaScript/Python integration) ---\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "    js = Javascript('''\n",
        "        async function takePhoto(quality) {\n",
        "          const div = document.createElement('div');\n",
        "          const video = document.createElement('video');\n",
        "          video.style.display = 'block';\n",
        "          const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "          document.body.appendChild(div);\n",
        "          div.appendChild(video);\n",
        "          video.srcObject = stream;\n",
        "          await video.play();\n",
        "\n",
        "          // Wait for the video to load and then set the canvas size\n",
        "          await new Promise(resolve => video.onloadedmetadata = resolve);\n",
        "\n",
        "          const canvas = document.createElement('canvas');\n",
        "          canvas.width = video.videoWidth;\n",
        "          canvas.height = video.videoHeight;\n",
        "          const context = canvas.getContext('2d');\n",
        "\n",
        "          // Add a button to capture the image\n",
        "          const button = document.createElement('button');\n",
        "          button.textContent = 'Capture';\n",
        "          div.appendChild(button);\n",
        "\n",
        "          await new Promise(resolve => button.onclick = resolve);\n",
        "\n",
        "          context.drawImage(video, 0, 0);\n",
        "          stream.getVideoTracks()[0].stop();\n",
        "          div.remove();\n",
        "          return canvas.toDataURL('image/jpeg', quality);\n",
        "        }\n",
        "        ''')\n",
        "    display(js)\n",
        "    data = eval_js('takePhoto({})'.format(quality))\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    return filename\n",
        "\n",
        "# --- 3. Preprocessing and Prediction Functions ---\n",
        "\n",
        "def preprocess_face(face_img):\n",
        "    \"\"\"Resizes and normalizes the detected face for the model.\"\"\"\n",
        "    gray_face = cv2.cvtColor(face_img, cv2.COLOR_BGR2GRAY)\n",
        "    resized_face = cv2.resize(gray_face, (48, 48), interpolation=cv2.INTER_AREA)\n",
        "    # Normalize and reshape for the model (48x48x1)\n",
        "    normalized_face = resized_face.astype('float32') / 255.0\n",
        "    input_face = np.expand_dims(normalized_face, axis=0) # Add batch dimension\n",
        "    input_face = np.expand_dims(input_face, axis=-1) # Add channel dimension\n",
        "    return input_face\n",
        "\n",
        "def predict_emotion_from_photo(filename='captured_face.jpg'):\n",
        "    \"\"\"Captures photo, detects face, and predicts emotion.\"\"\"\n",
        "    try:\n",
        "        model = load_model(model_filename)\n",
        "    except Exception as e:\n",
        "        print(f\"FATAL ERROR: Could not load model '{model_filename}'. Ensure training was successful. Error: {e}\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nüì∏ **Action Required:** Camera opening. Please click 'Capture' when ready! üì∏\")\n",
        "\n",
        "    # Capture the photo\n",
        "    try:\n",
        "        image_file = take_photo(filename)\n",
        "        print(f'Saved photo to {image_file}')\n",
        "    except Exception as err:\n",
        "        print(f\"Camera capture failed: {str(err)}\")\n",
        "        return\n",
        "\n",
        "    # Read and process the image\n",
        "    img = cv2.imread(image_file)\n",
        "    if img is None:\n",
        "        print(f\"Error: Could not read image file {image_file}\")\n",
        "        return\n",
        "\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "    if len(faces) == 0:\n",
        "        text = \"No face detected in the photo.\"\n",
        "        cv2.putText(img, text, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "        print(f\"‚ö†Ô∏è {text}\")\n",
        "    else:\n",
        "        for (x, y, w, h) in faces:\n",
        "            face_img = img[y:y+h, x:x+w]\n",
        "            input_face = preprocess_face(face_img)\n",
        "\n",
        "            # Predict emotion\n",
        "            predictions = model.predict(input_face, verbose=0)\n",
        "            predicted_class_index = np.argmax(predictions[0])\n",
        "            predicted_emotion = emotion_labels[predicted_class_index]\n",
        "            confidence = predictions[0][predicted_class_index] * 100\n",
        "\n",
        "            # Draw results on the image\n",
        "            color = (0, 255, 0) if predicted_emotion == 'Happy' else (0, 0, 255)\n",
        "            label = f\"{predicted_emotion}: {confidence:.2f}%\"\n",
        "            cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
        "            cv2.putText(img, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
        "\n",
        "            # Output Result\n",
        "            if predicted_emotion == 'Happy':\n",
        "                print(f\"ü•≥ **PREDICTION:** You look **{predicted_emotion}** with **{confidence:.2f}%** confidence!\")\n",
        "            else:\n",
        "                print(f\"ü§î **PREDICTION:** The detected emotion is **{predicted_emotion}** ({confidence:.2f}%).\")\n",
        "\n",
        "    # Display the final image with results\n",
        "    print(\"\\nüñºÔ∏è Displaying result:\")\n",
        "    cv2_imshow(img)\n",
        "\n",
        "# --- 4. EXECUTE THE DETECTION PROCESS ---\n",
        "predict_emotion_from_photo()"
      ]
    }
  ]
}